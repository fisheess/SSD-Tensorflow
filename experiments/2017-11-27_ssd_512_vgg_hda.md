## Training ssd_512_vgg on hda only, 2 classes, 27.11.2017
**Training**
original training params:
```bash
python train_ssd_network.py \
    --train_dir=/home/dst/SSD/experiments/hda_27-11-2017/logs \
    --dataset_dir=/home/dst/SSD/hda_tfrecords \
    --dataset_name=hda \
    --dataset_splict_name=train \
    --model_name=ssd_512_vgg \
    --checkpoint_path=/home/dst/SSD/checkpoints/vgg_16.ckpt \
    --learning_rate=0.001 \
    --learning_rate_decay_type='fixed' \
    --batch_size=10 \
    --num_classes=2 \
    --checkpoint_model_scope=vgg_16 \
    --checkpoint_exclude_scopes=ssd_512_vgg/conv6,ssd_512_vgg/conv7,ssd_512_vgg/block8,ssd_512_vgg/block9,ssd_512_vgg/block10,ssd_512_vgg/block11,ssd_512_vgg/block12,ssd_512_vgg/block4_box,ssd_512_vgg/block7_box,ssd_512_vgg/block8_box,ssd_512_vgg/block9_box,ssd_512_vgg/block10_box,ssd_512_vgg/block11_box,ssd_512_vgg/block12_box \
    --trainable_scopes=ssd_512_vgg/conv6,ssd_512_vgg/conv7,ssd_512_vgg/block8,ssd_512_vgg/block9,ssd_512_vgg/block10,ssd_512_vgg/block11,ssd_512_vgg/block12,ssd_512_vgg/block4_box,ssd_512_vgg/block7_box,ssd_512_vgg/block8_box,ssd_512_vgg/block9_box,ssd_512_vgg/block10_box,ssd_512_vgg/block11_box,ssd_512_vgg/block12_box \
    --save_summaries_secs=30 \
    --optimizer=adam \
    --num_clones=4 \
    --gpu_momory_fraction=0.9 \
    --weight_decay=0.0005 \
    --max_number_of_steps=1361
```
mAP = 0
loss drops very slowly after 400 steps, weights do not change, try reducing batch_size:
```bash
python train_ssd_network.py \
    --train_dir=/home/dst/SSD/experiments/hda_27-11-2017/logs \
    --dataset_dir=/home/dst/SSD/hda_tfrecords \
    --dataset_name=hda \
    --dataset_splict_name=train \
    --model_name=ssd_512_vgg \
    --checkpoint_path=/home/dst/SSD/checkpoints/vgg_16.ckpt \
    --learning_rate=0.001 \
    --learning_rate_decay_type='fixed' \
    --batch_size=2 \
    --num_classes=2 \
    --checkpoint_model_scope=vgg_16 \
    --checkpoint_exclude_scopes=ssd_512_vgg/conv6,ssd_512_vgg/conv7,ssd_512_vgg/block8,ssd_512_vgg/block9,ssd_512_vgg/block10,ssd_512_vgg/block11,ssd_512_vgg/block12,ssd_512_vgg/block4_box,ssd_512_vgg/block7_box,ssd_512_vgg/block8_box,ssd_512_vgg/block9_box,ssd_512_vgg/block10_box,ssd_512_vgg/block11_box,ssd_512_vgg/block12_box \
    --trainable_scopes=ssd_512_vgg/conv6,ssd_512_vgg/conv7,ssd_512_vgg/block8,ssd_512_vgg/block9,ssd_512_vgg/block10,ssd_512_vgg/block11,ssd_512_vgg/block12,ssd_512_vgg/block4_box,ssd_512_vgg/block7_box,ssd_512_vgg/block8_box,ssd_512_vgg/block9_box,ssd_512_vgg/block10_box,ssd_512_vgg/block11_box,ssd_512_vgg/block12_box \
    --save_summaries_secs=30 \
    --optimizer=adam \
    --num_clones=4 \
    --gpu_momory_fraction=0.9 \
    --weight_decay=0.0005 \
    --max_number_of_steps=3000
```
mAP = 0
small batch_size does not help, try raising learning_rate to 0.01
```bash
python train_ssd_network.py \
    --train_dir=/home/dst/SSD/experiments/hda_27-11-2017/logs \
    --dataset_dir=/home/dst/SSD/hda_tfrecords \
    --dataset_name=hda \
    --dataset_splict_name=train \
    --model_name=ssd_512_vgg \
    --checkpoint_path=/home/dst/SSD/checkpoints/vgg_16.ckpt \
    --learning_rate=2 \
    --learning_rate_decay_type='fixed' \
    --batch_size=10 \
    --num_classes=2 \
    --checkpoint_model_scope=vgg_16 \
    --checkpoint_exclude_scopes=ssd_512_vgg/conv6,ssd_512_vgg/conv7,ssd_512_vgg/block8,ssd_512_vgg/block9,ssd_512_vgg/block10,ssd_512_vgg/block11,ssd_512_vgg/block12,ssd_512_vgg/block4_box,ssd_512_vgg/block7_box,ssd_512_vgg/block8_box,ssd_512_vgg/block9_box,ssd_512_vgg/block10_box,ssd_512_vgg/block11_box,ssd_512_vgg/block12_box \
    --trainable_scopes=ssd_512_vgg/conv6,ssd_512_vgg/conv7,ssd_512_vgg/block8,ssd_512_vgg/block9,ssd_512_vgg/block10,ssd_512_vgg/block11,ssd_512_vgg/block12,ssd_512_vgg/block4_box,ssd_512_vgg/block7_box,ssd_512_vgg/block8_box,ssd_512_vgg/block9_box,ssd_512_vgg/block10_box,ssd_512_vgg/block11_box,ssd_512_vgg/block12_box \
    --save_summaries_secs=30 \
    --optimizer=adam \
    --num_clones=4 \
    --gpu_momory_fraction=0.9 \
    --weight_decay=0.0005 \
    --max_number_of_steps=10000
```
mAP = 0
raise lr further to 0.1 till 4.5k steps, mAP = 0, no fluctuations of loss observed
raise lr further to 0.5 till 5k steps, mAP = 0, no fluctuations of loss observed
lr = 0.8 till 5.5k steps, mAP = 0, no fluctuations of loss observed, Loss drops at even more consistent speed
lr = 0.9 till 6k steps, mAP = 0, no fluctuations of loss observed, loss drops slower than lr = 0.8
lr = 0.8 till 7380 steps, loss slows down
lr = 0.9 till 8770 steps, loss now drops very slowly, mAP = 0
lr = 0.99 till 9.5k
lr = 2 and training still goes!? 10k steps, loss=0.38, mAP = 0
Looking at the histograms, weights changed yet converging almost all to 0.
I can guess nothing at the moment.

**Test**
```bash
python eval_ssd_network.py \
    --eval_dir=/home/dst/SSD/experiments/hda_27-11-2017/eval\
    --dataset_dir=/home/dst/SSD/hda_tfrecords \
    --dataset_name=hda \
    --num_classes=2 \
    --dataset_split_name=train \
    --model_name=ssd_512_vgg \
    --checkpoint_path=/home/dst/SSD/experiments/hda_27-11-2017/logs \
    --wait_for_checkpoints=False \
    --batch_size=1 \
    --max_num_batches=500
```